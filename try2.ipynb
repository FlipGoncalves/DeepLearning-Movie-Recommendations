{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab9f625a",
   "metadata": {},
   "source": [
    "# Deep Learning Algorithms for Movie Recommendations\n",
    "## Filipe Gon√ßalves, 98083\n",
    "## Pedro Lopes, 97827"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "78d32359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "from IPython import display\n",
    "import sklearn\n",
    "import sklearn.manifold\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8dc5eb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the users and ratings datasets\n",
    "\n",
    "users_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']\n",
    "users = pd.read_csv('./ml-100k/u.user', sep='|', names=users_cols)\n",
    "\n",
    "ratings_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "ratings = pd.read_csv('./ml-100k/u.data', sep='\\t', names=ratings_cols, encoding='latin-1')\n",
    "\n",
    "genre_cols = [\n",
    "    \"genre_unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children\", \"Comedy\",\n",
    "    \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\",\n",
    "    \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"]\n",
    "    \n",
    "movies_cols = ['movie_id', 'title', 'release_date', 'video_release_date', 'imdb_url'] + genre_cols\n",
    "movies = pd.read_csv('./ml-100k/u.item', sep='|', names=movies_cols, encoding='latin-1')\n",
    "#movies = movies.drop(columns=['release_date', 'video_release_date', 'imdb_url'])\n",
    "\n",
    "#print(users)\n",
    "#print(ratings)\n",
    "#print(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c0351774",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['year'] = movies['release_date'].apply(lambda x: str(x).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "528e41cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, holdout_fraction=0.1):\n",
    "    \n",
    "    test = df.sample(frac=0.1, replace=False)\n",
    "    train = df[~df.index.isin(test.index)]\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b070e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the rated movies with the user id\n",
    "# https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/\n",
    "\n",
    "rated_movies = (ratings[[\"user_id\", \"movie_id\"]]\n",
    "                .groupby(\"user_id\", as_index=False)\n",
    "                .aggregate(lambda x: list(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "16b5a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the individual tensors for the batch input of the Neural Network model\n",
    "\n",
    "yearsDict = {\n",
    "    movie: year for movie, year in zip(movies[\"movie_id\"], movies[\"year\"])\n",
    "}\n",
    "genreDict = {}\n",
    "active = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ea735972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the available genre for that particular movie\n",
    "\n",
    "def foo_(movies, genres):\n",
    "    \n",
    "    def all_genres(gs):\n",
    "        active = [genre for genre, g in zip(genres, gs) if g==1]\n",
    "        return '-'.join(active)\n",
    "    \n",
    "    movies['all_genres'] = [all_genres(gs) for gs in zip(*[movies[genre] for genre in genres])]\n",
    "\n",
    "    \n",
    "foo_(movies, genre_cols)\n",
    "genres_dict = {movie: genres.split('-') for movie, genres in zip(movies[\"movie_id\"], movies[\"all_genres\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "dbe6ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#or movie_id in movie_ids for x in genres_dict[movie_id]\n",
    "\n",
    "movie = []\n",
    "year = []\n",
    "genre = []\n",
    "label = []\n",
    "\n",
    "for movie_ids in ratings['movie_id'].values:\n",
    "    movie.append(movie_ids)\n",
    "    genre.append([x for x in genres_dict[movie_ids]])\n",
    "    \n",
    "#genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ec3a4566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(ratings, batch_size):\n",
    "\n",
    "    # Function to fill null values to form sparse tensor\n",
    "    def pad(x, fill):\n",
    "        return pd.DataFrame.from_dict(x).fillna(fill).values\n",
    "\n",
    "    movie = []\n",
    "    year = []\n",
    "    genre = []\n",
    "    label = []\n",
    "\n",
    "      # Fill the input with 4 features\n",
    "    for movie_ids in ratings[\"movie_id\"].values:\n",
    "        movie.append([chr(movie_id) for movie_id in movie_ids])\n",
    "        genre.append([x for movie_id in movie_ids for x in genres_dict[movie_id]])\n",
    "        year.append([yearsDict[movie_id][0] for movie_id in movie_ids])\n",
    "        label.append([chr(movie_id) for movie_id in movie_ids])\n",
    "\n",
    "    # Creating the input tensors\n",
    "    features = {\n",
    "        \"movie_id\": pad(movie, \"\"),\n",
    "        \"year\": pad(year, \"\"),\n",
    "        \"genre\": pad(genre, \"\"),\n",
    "        \"label\": pad(label, \"\")\n",
    "    }\n",
    "  \n",
    "    # Creating a single batch for each iteraton\n",
    "    batch = (\n",
    "        tf.data.Dataset.from_tensor_slices(features)\n",
    "        .shuffle(1000)\n",
    "        .repeat()\n",
    "        .batch(batch_size)\n",
    "        # one_shot_iterator only in TF1.X\n",
    "        #.make_one_shot_iterator()\n",
    "        #.get_next()\n",
    "        )\n",
    "  \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "995f5073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(user_embeddings, movie_embeddings, labels):\n",
    "  \n",
    "    # Verify that the embddings have compatible dimensions\n",
    "    user_emb_dim = user_embeddings.shape[1].value\n",
    "    movie_emb_dim = movie_embeddings.shape[1].value\n",
    "    logits = tf.matmul(user_embeddings, movie_embeddings, transpose_b=True)\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=labels))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "af8afd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training class\n",
    "\n",
    "class Model(object):\n",
    "\n",
    "    def __init__(self, embedding_vars, loss, metrics=None):\n",
    "        self.embedding_vars = embedding_vars\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        self.embeddings = {k: None for k in embedding_vars}\n",
    "        self.session = None\n",
    "\n",
    "    def embeddings(self):\n",
    "        return self.embeddings\n",
    "\n",
    "    def train(self, num_iterations=100, learning_rate=1.0, plot_results=True, optimizer=tf.keras.optimizers.SGD()):\n",
    "        with self.loss.graph.as_default():\n",
    "\n",
    "            # Minimize loss function\n",
    "            train_op = optimizer(learning_rate).minimize(self.loss)\n",
    "\n",
    "            # Initialise the operation\n",
    "            local_init_op = tf.group(tf.variables_initializer(optimizer(learning_rate).variables()), tf.local_variables_initializer())\n",
    "\n",
    "            if self.session is None:\n",
    "                self.session = tf.Session()\n",
    "\n",
    "            with self.session.as_default():\n",
    "                self.session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "\n",
    "        with self.session.as_default():\n",
    "            local_init_op.run()\n",
    "            iterations = []\n",
    "            metrics = {}\n",
    "            metrics_vals = {}\n",
    "\n",
    "        # Train and append results.\n",
    "        for i in range(num_iterations + 1):\n",
    "            _, results = self.session.run((train_op, metrics))\n",
    "            if i == num_iterations:\n",
    "                for metric_val, result in zip(metrics_vals, results):\n",
    "                    # Embeddings are u and k respectively\n",
    "                    for k, v in result.items():\n",
    "                        metric_val[k].append(v)\n",
    "\n",
    "        for k, v in self.embedding_vars.items():\n",
    "            self.embeddings[k] = v.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fc22aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_softmax_model(rated_movies, embedding_cols, hidden_dims):\n",
    "    \n",
    "    def create_network(features):\n",
    "        # Create a bag-of-words embedding for each sparse feature.\n",
    "        inputs = tf.compat.v1.feature_column.input_layer(features, embedding_cols)\n",
    "        # Hidden layers.\n",
    "        input_dim = inputs.shape[1].value\n",
    "        for i, output_dim in enumerate(hidden_dims):\n",
    "            w = tf.get_variable(\n",
    "                \"hidden%d_w_\" % i, shape=[input_dim, output_dim],\n",
    "                initializer=tf.truncated_normal_initializer(\n",
    "                    stddev=1./np.sqrt(output_dim))) / 10.\n",
    "            \n",
    "            outputs = tf.matmul(inputs, w)\n",
    "            input_dim = output_dim\n",
    "            inputs = outputs\n",
    "        return outputs\n",
    "\n",
    "    train_rated_movies, test_rated_movies = split_dataframe(rated_movies)\n",
    "    train_batch = make_batch(train_rated_movies, 200)\n",
    "    test_batch = make_batch(test_rated_movies, 100)\n",
    "\n",
    "    with tf.compat.v1.variable_scope(\"model\", reuse=False):\n",
    "        # Train\n",
    "        train_user_embeddings = create_network(train_batch)\n",
    "        train_labels = select_random(train_batch[\"label\"])\n",
    "    with tf.compat.v1.variable_scope(\"model\", reuse=True):\n",
    "        # Test\n",
    "        test_user_embeddings = create_network(test_batch)\n",
    "        test_labels = select_random(test_batch[\"label\"])\n",
    "        movie_embeddings = tf.get_variable(\n",
    "            \"input_layer/movie_id_embedding/embedding_weights\")\n",
    "\n",
    "    test_loss = softmax_loss(\n",
    "      test_user_embeddings, movie_embeddings, test_labels)\n",
    "    train_loss = softmax_loss(\n",
    "      train_user_embeddings, movie_embeddings, train_labels)\n",
    "    _, test_precision_at_10 = tf.metrics.precision_at_k(\n",
    "      labels=test_labels,\n",
    "      predictions=tf.matmul(test_user_embeddings, movie_embeddings, transpose_b=True),\n",
    "      k=10)\n",
    "\n",
    "    metrics = (\n",
    "      {\"train_loss\": train_loss, \"test_loss\": test_loss},\n",
    "      {\"test_precision_at_10\": test_precision_at_10}\n",
    "    )\n",
    "    embeddings = {\"movie_id\": movie_embeddings}\n",
    "    return CFModel(embeddings, train_loss, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3c4a8771",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BatchDataset' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-8cdc9fa21eb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Initialise graph for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     softmax_model = build_softmax_model(\n\u001b[0m\u001b[1;32m      9\u001b[0m                   \u001b[0mrated_movies\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                   embedding_cols=[ # Embeddings learned\n",
      "\u001b[0;32m<ipython-input-154-65c4cd8c1dd1>\u001b[0m in \u001b[0;36mbuild_softmax_model\u001b[0;34m(rated_movies, embedding_cols, hidden_dims)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtrain_user_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-154-65c4cd8c1dd1>\u001b[0m in \u001b[0;36mcreate_network\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# Create a bag-of-words embedding for each sparse feature.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Hidden layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/tensorflow/python/feature_column/feature_column.py\u001b[0m in \u001b[0;36minput_layer\u001b[0;34m(features, feature_columns, weight_collections, trainable, cols_to_vars, cols_to_output_tensors)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0man\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0m_DenseColumn\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m   \"\"\"\n\u001b[0;32m--> 292\u001b[0;31m   return _internal_input_layer(\n\u001b[0m\u001b[1;32m    293\u001b[0m       \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m       \u001b[0mfeature_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/tensorflow/python/feature_column/feature_column.py\u001b[0m in \u001b[0;36m_internal_input_layer\u001b[0;34m(features, feature_columns, weight_collections, trainable, cols_to_vars, scope, cols_to_output_tensors, from_template)\u001b[0m\n\u001b[1;32m    223\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     with variable_scope.variable_scope(\n\u001b[0;32m--> 225\u001b[0;31m         scope, default_name='input_layer', values=features.values()):\n\u001b[0m\u001b[1;32m    226\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_get_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BatchDataset' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "# Create feature embedding columns\n",
    "def make_embedding_col(key, embedding_dim):\n",
    "    categorical_col = tf.feature_column.categorical_column_with_vocabulary_list(key=key, vocabulary_list=list(set(movies[key].values)), num_oov_buckets=0)\n",
    "    return tf.feature_column.embedding_column(categorical_column=categorical_col, dimension=embedding_dim, combiner='mean')\n",
    "\n",
    "# Initialise graph for training \n",
    "with tf.Graph().as_default():\n",
    "    softmax_model = build_softmax_model(\n",
    "                  rated_movies,  # Input\n",
    "                  embedding_cols=[ # Embeddings learned\n",
    "                  make_embedding_col(\"movie_id\", 35)],\n",
    "                  hidden_dims=[35]) # Dimension of the embedding\n",
    "\n",
    "# Perform training \n",
    "softmax_model.train(learning_rate=8., num_iterations=3000, optimizer=tf.train.AdagradOptimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
