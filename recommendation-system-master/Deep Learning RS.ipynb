{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses deep learning (CNN, GRU, LSTM) to predict user ratings of movies based on user reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A11N155CW1UV02</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3BC8O2KCL29V2</td>\n",
       "      <td>I highly recommend this series. It is a must f...</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A60D5HQFOTSOM</td>\n",
       "      <td>This one is a real snoozer. Don't believe anyt...</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1RJPIGRSNX4PW</td>\n",
       "      <td>Mysteries are interesting.  The tension betwee...</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A16XRPF40679KG</td>\n",
       "      <td>This show always is excellent, as far as briti...</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID                                         reviewText  \\\n",
       "0  A11N155CW1UV02  I had big expectations because I love English ...   \n",
       "1  A3BC8O2KCL29V2  I highly recommend this series. It is a must f...   \n",
       "2   A60D5HQFOTSOM  This one is a real snoozer. Don't believe anyt...   \n",
       "3  A1RJPIGRSNX4PW  Mysteries are interesting.  The tension betwee...   \n",
       "4  A16XRPF40679KG  This show always is excellent, as far as briti...   \n",
       "\n",
       "         asin  overall  \n",
       "0  B000H00VBQ      2.0  \n",
       "1  B000H00VBQ      5.0  \n",
       "2  B000H00VBQ      1.0  \n",
       "3  B000H00VBQ      4.0  \n",
       "4  B000H00VBQ      5.0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_list_of_dicts(fname): \n",
    "    return [json.loads(i) for i in open(fname, \"rt\")]\n",
    "\n",
    "raw_data = get_list_of_dicts(\"Amazon_Instant_Video_5.json\")\n",
    "data = pd.DataFrame(raw_data).loc[:, [\"reviewerID\", \"reviewText\", \"asin\", \"overall\"]]\n",
    "\n",
    "data = data[:10000] # only 10 000 user reviews\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>userReviews</th>\n",
       "      <th>movieReviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A11N155CW1UV02</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>2.0</td>\n",
       "      <td></td>\n",
       "      <td>This show always is excellent, as far as briti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3BC8O2KCL29V2</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td></td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A60D5HQFOTSOM</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I watched this a couple of weeks ago. There ar...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1RJPIGRSNX4PW</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The acting was excellent.  The acting, the rel...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A16XRPF40679KG</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>As many people said this show kept getting bet...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  overall  \\\n",
       "0  A11N155CW1UV02  B000H00VBQ      2.0   \n",
       "1  A3BC8O2KCL29V2  B000H00VBQ      5.0   \n",
       "2   A60D5HQFOTSOM  B000H00VBQ      1.0   \n",
       "3  A1RJPIGRSNX4PW  B000H00VBQ      4.0   \n",
       "4  A16XRPF40679KG  B000H00VBQ      5.0   \n",
       "\n",
       "                                         userReviews  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2  I watched this a couple of weeks ago. There ar...   \n",
       "3  The acting was excellent.  The acting, the rel...   \n",
       "4  As many people said this show kept getting bet...   \n",
       "\n",
       "                                        movieReviews  \n",
       "0  This show always is excellent, as far as briti...  \n",
       "1  I had big expectations because I love English ...  \n",
       "2  I had big expectations because I love English ...  \n",
       "3  I had big expectations because I love English ...  \n",
       "4  I had big expectations because I love English ...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_user_reviews(x):\n",
    "    ur = user_reviews.loc[x[\"reviewerID\"]].drop(x[\"asin\"]).values.tolist()\n",
    "    mr = movie_reviews.loc[x[\"asin\"]].drop(x[\"reviewerID\"]).values.tolist()\n",
    "    x[\"userReviews\"] = \" \".join(list(map(lambda x: x[0], ur)))\n",
    "    x[\"movieReviews\"] = \" \".join(list(map(lambda x: x[0], mr)))\n",
    "    return x\n",
    "\n",
    "user_item_review = data.drop(\"reviewText\", axis=1)\n",
    "user_reviews = pd.pivot_table(data, index=[\"reviewerID\", \"asin\"], aggfunc=lambda x: x).drop(\"overall\", axis=1)  \n",
    "movie_reviews = pd.pivot_table(data, index=[\"asin\", \"reviewerID\"], aggfunc=lambda x: x).drop(\"overall\", axis=1)\n",
    "\n",
    "df = user_item_review.apply(add_user_reviews, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split \n",
    "test_size = 0.005\n",
    "\n",
    "# get test_size percentage of users\n",
    "unique_users = df.loc[:, \"reviewerID\"].unique()\n",
    "users_size = len(unique_users)\n",
    "test_idx = np.random.choice(users_size, size=int(users_size * test_size), replace=False)\n",
    "\n",
    "# get test users\n",
    "test_users = unique_users[test_idx]\n",
    "\n",
    "# everyone else is a training user\n",
    "train_users = np.delete(unique_users, test_idx)\n",
    "\n",
    "test = df[df[\"reviewerID\"].isin(test_users)]\n",
    "train = df[df[\"reviewerID\"].isin(train_users)]\n",
    "\n",
    "unique_test_movies = test[\"asin\"].unique()\n",
    "\n",
    "# drop the movies that also appear in our test set. In order to be\n",
    "# a true train/test split, we are forced to discard some data entirely\n",
    "train = train.where(np.logical_not(train[\"asin\"].isin(unique_test_movies))).dropna()\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed the reviews into GloVe word2vect model. \n",
    "\n",
    "The pre-trained GloVe model is downloadable at\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "# functions to embed user reviews into the GloVe word2vect model\n",
    "def init_embeddings_map(fname):\n",
    "    with open(os.path.join(\"glove.6B\", fname), encoding=\"utf8\") as glove:\n",
    "        return {l[0]: np.asarray(l[1:], dtype=\"float32\") for l in\n",
    "                [line.split() for line in glove]}\n",
    "\n",
    "def get_embed_func(i_len, u_len, pad_value, embedding_map):\n",
    "    def embed(row):\n",
    "        sentence = row[\"userReviews\"].split()[:u_len]\n",
    "        reviews = list(map(lambda word: embedding_map.get(word)\n",
    "            if word in embedding_map else pad_value, sentence))\n",
    "        row[\"userReviews\"] = reviews +[pad_value] * (u_len - len(reviews))\n",
    "        sentence = row[\"movieReviews\"].split()[:i_len]\n",
    "        reviews = list(map(lambda word: embedding_map.get(word) if word in embedding_map else pad_value, sentence))\n",
    "        row[\"movieReviews\"] = reviews +[pad_value] * (i_len - len(reviews))\n",
    "        return row\n",
    "    return embed\n",
    "\n",
    "print(\"Before Embedding\")\n",
    "emb_size = 50 #or 100, 200, 300\n",
    "embedding_map = init_embeddings_map(\"glove.6b.\" + str(emb_size) + \"d.10000.txt\")\n",
    "print(\"After Embedding\")\n",
    "\n",
    "print(\"Before Apply\")\n",
    "user_sizes = df.loc[:, \"userReviews\"].apply(lambda x: x.split()).apply(len)\n",
    "item_sizes = df.loc[:, \"movieReviews\"].apply(lambda x: x.split()).apply(len)\n",
    "print(\"After Apply\")\n",
    "\n",
    "u_ptile = 40\n",
    "i_ptile = 15\n",
    "u_len = int(np.percentile(user_sizes, u_ptile))\n",
    "i_len = int(np.percentile(item_sizes, i_ptile))\n",
    "\n",
    "print(\"Before Embedding Function\")\n",
    "embedding_fn = get_embed_func(i_len, u_len, np.array([0.0] * emb_size), embedding_map)\n",
    "print(\"After Embedding Function\")\n",
    "\n",
    "print(\"Before Embedding Train\")\n",
    "train_embedded = train.apply(embedding_fn, axis=1)\n",
    "test_embedded = test.apply(embedding_fn, axis=1)\n",
    "print(\"After Embedding Train\")\n",
    "\n",
    "print(u_len, i_len) # size of input in deep neural networks, useful to set parameters\n",
    "train_embedded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.layers import Conv1D, GRU, LSTM, MaxPooling1D, Flatten\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.layers import Add, Dot, Concatenate\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def cnn_tower(max_len, embedding_size, hidden_size, filters=4, kernel_size=10):\n",
    "        input_layer = Input(shape=(max_len, embedding_size))\n",
    "        tower = Conv1D(filters=filters, kernel_size=kernel_size, activation=\"tanh\")(input_layer)\n",
    "        tower = MaxPooling1D()(tower)\n",
    "        tower = Conv1D(filters=filters, kernel_size=kernel_size, activation=\"tanh\")(tower)\n",
    "        tower = MaxPooling1D()(tower)\n",
    "        tower = Flatten()(tower)\n",
    "        tower = Dense(hidden_size, activation=\"relu\")(tower)\n",
    "        tower = Dropout(0.4)(tower)\n",
    "        return input_layer, tower\n",
    "    \n",
    "def CNN_model(embedding_size, hidden_size, u_len, i_len):\n",
    "    inputU, towerU = cnn_tower(u_len, embedding_size, hidden_size)\n",
    "    inputM, towerM = cnn_tower(i_len, embedding_size, hidden_size)\n",
    "    joined = Concatenate()([towerU, towerM])\n",
    "    outNeuron = Dense(1)(joined)\n",
    "    dotproduct = Dot(axes=1)([towerU, towerM])\n",
    "    output_layer = Add()([outNeuron, dotproduct])\n",
    "        \n",
    "    model = Model(inputs=[inputU, inputM], outputs=[output_layer])\n",
    "    return model\n",
    "\n",
    "hidden_size = 64\n",
    "\n",
    "model_cnn = CNN_model(emb_size, hidden_size, u_len, i_len)\n",
    "model_cnn.compile(optimizer='Adam', loss='mse')\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "user_reviews = np.array(list(train_embedded.loc[:, \"userReviews\"]))\n",
    "movie_reviews = np.array(list(train_embedded.loc[:, \"movieReviews\"]))\n",
    "\n",
    "train_inputs = [user_reviews, movie_reviews]\n",
    "train_outputs = train_embedded.loc[:, \"overall\"]\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"cnn_log\")\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "checkpoint = ModelCheckpoint('cnn_weights.{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', save_best_only=True)\n",
    "train_history = model_cnn.fit(train_inputs, train_outputs, callbacks=[tensorboard, earlystop, checkpoint], \n",
    "                              validation_split=0.05, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "model_cnn.save(\"cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_cnn.history.history['loss'])\n",
    "plt.plot(model_cnn.history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_tower(max_len, embedding_size, hidden_size, rnn_hidden_size, filters=2, kernel_size=8):\n",
    "        input_layer = Input(shape=(max_len, embedding_size))\n",
    "        tower = GRU(rnn_hidden_size, activation=\"tanh\")(input_layer)\n",
    "        tower = Dense(hidden_size, activation=\"relu\")(tower)\n",
    "        tower = Dropout(0.4)(tower)\n",
    "        return input_layer, tower\n",
    "    \n",
    "def GRU_model(embedding_size, hidden_size, rnn_hidden_size, u_len, i_len):\n",
    "    inputU, towerU = gru_tower(u_len, embedding_size, hidden_size, rnn_hidden_size)\n",
    "    inputM, towerM = gru_tower(i_len, embedding_size, hidden_size, rnn_hidden_size)\n",
    "    joined = Concatenate()([towerU, towerM])\n",
    "    outNeuron = Dense(1)(joined)\n",
    "    dotproduct = Dot(axes=1)([towerU, towerM])\n",
    "    output_layer = Add()([outNeuron, dotproduct])\n",
    "        \n",
    "    model = Model(inputs=[inputU, inputM], outputs=[output_layer])\n",
    "    return model\n",
    "\n",
    "hidden_size = 64\n",
    "rnn_hidden_size = 64\n",
    "\n",
    "model_gru = GRU_model(emb_size, hidden_size, rnn_hidden_size, u_len, i_len)\n",
    "model_gru.compile(optimizer='Adam', loss='mse')\n",
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"gru_log\")\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "checkpoint = ModelCheckpoint('gru_weights.{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', save_best_only=True)\n",
    "train_history = model_gru.fit(train_inputs, train_outputs, callbacks=[tensorboard, earlystop, checkpoint], \n",
    "                              validation_split=0.05, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "model_gru.save(\"gru.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_gru.history.history['loss'])\n",
    "plt.plot(model_gru.history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model\n",
    "def lstm_tower(max_len, embedding_size, hidden_size, rnn_hidden_size, filters=2, kernel_size=8):\n",
    "        input_layer = Input(shape=(max_len, embedding_size))\n",
    "        tower = LSTM(rnn_hidden_size, activation=\"tanh\")(input_layer)\n",
    "        tower = Dense(hidden_size, activation=\"relu\")(tower)\n",
    "        tower = Dropout(0.4)(tower)\n",
    "        return input_layer, tower\n",
    "    \n",
    "def LSTM_model(embedding_size, hidden_size, rnn_hidden_size, u_len, i_len):\n",
    "    inputU, towerU = lstm_tower(u_len, embedding_size, hidden_size, rnn_hidden_size)\n",
    "    inputM, towerM = lstm_tower(i_len, embedding_size, hidden_size, rnn_hidden_size)\n",
    "    joined = Concatenate()([towerU, towerM])\n",
    "    outNeuron = Dense(1)(joined)\n",
    "    dotproduct = Dot(axes=1)([towerU, towerM])\n",
    "    output_layer = Add()([outNeuron, dotproduct])\n",
    "        \n",
    "    model = Model(inputs=[inputU, inputM], outputs=[output_layer])\n",
    "    return model\n",
    "\n",
    "\n",
    "hidden_size = 64\n",
    "rnn_hidden_size = 64\n",
    "\n",
    "model_lstm = LSTM_model(emb_size, hidden_size, rnn_hidden_size, u_len, i_len)\n",
    "model_lstm.compile(optimizer='Adam', loss='mse')\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 30\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"lstm_log\")\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "checkpoint = ModelCheckpoint('lstm_weights.{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', save_best_only=True)\n",
    "train_history = model_lstm.fit(train_inputs, train_outputs, callbacks=[tensorboard, earlystop, checkpoint], \n",
    "                              validation_split=0.05, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "model_lstm.save(\"lstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_lstm.history.history['loss'])\n",
    "plt.plot(model_lstm.history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction and errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_reviews = np.array(list(test_embedded.loc[:, \"userReviews\"]))\n",
    "movie_reviews = np.array(list(test_embedded.loc[:, \"movieReviews\"]))\n",
    "test_inputs = [user_reviews, movie_reviews]\n",
    "\n",
    "true_rating = np.array(list(test_embedded.loc[:, \"overall\"])).reshape((-1, 1))\n",
    "\n",
    "predictions_cnn = model_cnn.predict(test_inputs)\n",
    "predictions_gru = model_gru.predict(test_inputs)\n",
    "predictions_lstm = model_lstm.predict(test_inputs)\n",
    "\n",
    "error_cnn = np.square(predictions_cnn - true_rating)\n",
    "print(\"Test MSE for CNN model :\", np.average(error_cnn))\n",
    "\n",
    "error_gru = np.square(predictions_gru - true_rating)\n",
    "print(\"Test MSE for GRU model :\", np.average(error_gru))\n",
    "\n",
    "error_lstm = np.square(predictions_lstm - true_rating)\n",
    "print(\"Test MSE for LSTM model :\", np.average(error_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movies(data):\n",
    "    data_movies = data[\"asin\"]\n",
    "    return list(set(data_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_recommendations(predict, movies):\n",
    "    num_recommendations = 10\n",
    "    dist = [(i+1, predict[i]) for i in range(len(predict))]\n",
    "    dist = sorted(dist, key=lambda x:x[1])[::-1]\n",
    "    for i in range(num_recommendations):\n",
    "        print(f\"Recommended movie: {movies[dist[i][0]]}, overall: {dist[i][1][0]}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN\n",
      "Recommended movie: B0047G01H0, overall: 4.48649263381958\n",
      "Recommended movie: B000IKP5AM, overall: 4.480325698852539\n",
      "Recommended movie: B004RZMQCE, overall: 4.357715129852295\n",
      "Recommended movie: B001ENLHX6, overall: 4.352227210998535\n",
      "Recommended movie: B003ZHOWFY, overall: 4.350558280944824\n",
      "Recommended movie: B000MVN8HE, overall: 4.312946319580078\n",
      "Recommended movie: B001VT4L7W, overall: 4.311435699462891\n",
      "Recommended movie: B0051HIK04, overall: 4.281321048736572\n",
      "Recommended movie: B001CMQH5M, overall: 4.229222774505615\n",
      "Recommended movie: B000ULZLYY, overall: 4.228974342346191\n",
      "GRU\n",
      "Recommended movie: B003ZHOWFY, overall: 5.022066593170166\n",
      "Recommended movie: B001RPORJ2, overall: 4.871674537658691\n",
      "Recommended movie: B0057UGEUS, overall: 4.8230133056640625\n",
      "Recommended movie: B000ULZLYY, overall: 4.795764446258545\n",
      "Recommended movie: B000MMX5E4, overall: 4.7846269607543945\n",
      "Recommended movie: B003HIC3ZW, overall: 4.7346343994140625\n",
      "Recommended movie: B004JM9BXM, overall: 4.7027506828308105\n",
      "Recommended movie: B004IY9DQQ, overall: 4.663974285125732\n",
      "Recommended movie: B003EYBOBS, overall: 4.647857189178467\n",
      "Recommended movie: B000MVN8HE, overall: 4.636935234069824\n",
      "LSTM\n",
      "Recommended movie: B003ZHOWFY, overall: 4.5428996086120605\n",
      "Recommended movie: B000MMX5E4, overall: 4.340725898742676\n",
      "Recommended movie: B000W4WD40, overall: 4.32950496673584\n",
      "Recommended movie: B001VT4L7W, overall: 4.3034467697143555\n",
      "Recommended movie: B003HIC3ZW, overall: 4.291642665863037\n",
      "Recommended movie: B004H75GBU, overall: 4.274750709533691\n",
      "Recommended movie: B0057UGEUS, overall: 4.271670341491699\n",
      "Recommended movie: B003CXP5O8, overall: 4.2664690017700195\n",
      "Recommended movie: B000J1B1PI, overall: 4.263984203338623\n",
      "Recommended movie: B004EKF0QQ, overall: 4.263718605041504\n"
     ]
    }
   ],
   "source": [
    "data_movies = get_movies(data)\n",
    "\n",
    "print(\"CNN\")\n",
    "best_recommendations(predictions_cnn, data_movies)\n",
    "\n",
    "print(\"GRU\")\n",
    "best_recommendations(predictions_gru, data_movies)\n",
    "\n",
    "print(\"LSTM\")\n",
    "best_recommendations(predictions_lstm, data_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_cnn  = np.round(predictions_cnn,1)\n",
    "predictions_gru  = np.round(predictions_gru,1)\n",
    "predictions_lstm = np.round(predictions_lstm,1)\n",
    "\n",
    "\n",
    "predictions_cnn  = np.array([x[0] for x in predictions_cnn])\n",
    "predictions_gru  = np.array([x[0] for x in predictions_gru])\n",
    "predictions_lstm = np.array([x[0] for x in predictions_lstm])\n",
    "true_rating =      np.array([x[0] for x in true_rating])\n",
    "\n",
    "#Here we want movies to be ranked the same or better than the real rating\n",
    "true_positives_cnn = 0\n",
    "true_positives_gru= 0\n",
    "true_positives_lstm = 0\n",
    "false_positives_cnn = 0\n",
    "false_positives_gru = 0\n",
    "false_positives_lstm = 0\n",
    "\n",
    "false_negatives_cnn   = 0\n",
    "false_negatives_gru = 0\n",
    "false_negatives_lstm = 0\n",
    "\n",
    "\n",
    "for i in range(len(true_rating)):\n",
    "    # Count TP\n",
    "    if predictions_cnn[i] <= true_rating[i] + 0.2 and predictions_cnn[i] >= true_rating[i] - 0.2:\n",
    "        true_positives_cnn +=1\n",
    "\n",
    "    if predictions_lstm[i] <= true_rating[i] + 0.2 and predictions_lstm[i] >= true_rating[i] - 0.2:\n",
    "        true_positives_gru +=1\n",
    "    \n",
    "    if predictions_gru[i] <= true_rating[i] + 0.2 and predictions_gru[i] >= true_rating[i] - 0.2:\n",
    "        true_positives_lstm +=1\n",
    "\n",
    "    # Count FP\n",
    "    if  predictions_cnn[i] > true_rating[i] + 0.2:\n",
    "       false_positives_cnn +=1\n",
    "    if  predictions_gru[i] > true_rating[i] + 0.2:\n",
    "       false_positives_gru +=1\n",
    "    if  predictions_lstm[i] > true_rating[i] + 0.2:\n",
    "       false_positives_lstm +=1\n",
    "\n",
    "    # Count FN\n",
    "    if  predictions_cnn[i]  <  true_rating[i] - 0.2:\n",
    "        false_negatives_cnn  += 1\n",
    "    if  predictions_cnn[i]  <  true_rating[i] - 0.2:\n",
    "        false_negatives_gru  += 1\n",
    "    if  predictions_cnn[i]  <  true_rating[i] - 0.2:\n",
    "        false_negatives_lstm += 1\n",
    "\n",
    "\n",
    "true_negatives = 0\n",
    "\n",
    "def calc_recall(true_positives, false_negatives):\n",
    "    return true_positives /( true_positives + false_negatives)\n",
    "\n",
    "def calc_precision(true_positives, false_positives):\n",
    "    return true_positives /( true_positives + false_positives)\n",
    "\n",
    "def f2_score(recall , precision):\n",
    "    return 5 * ( ( precision * recall ) / ((5 * precision) + recall ) )\n",
    "\n",
    "recall_cnn = calc_recall(true_positives_cnn, false_negatives_cnn) \n",
    "precision_cnn = calc_precision(true_positives_cnn, false_positives_cnn)\n",
    "\n",
    "cnn_f2_score =  f2_score(recall_cnn,precision_cnn)\n",
    "print(cnn_f2_score)\n",
    "\n",
    "\n",
    "\n",
    "recall_gru = calc_recall(true_positives_gru, false_negatives_gru) \n",
    "precision_gru = calc_precision(true_positives_gru, false_positives_gru)\n",
    "\n",
    "gru_f2_score =  f2_score(recall_gru,precision_gru)\n",
    "print(gru_f2_score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "recall_lstm = calc_recall(true_positives_lstm, false_negatives_lstm) \n",
    "precision_lstm = calc_precision(true_positives_lstm, false_positives_lstm)\n",
    "\n",
    "lstm_f2_score =  f2_score(recall_lstm, precision_lstm)\n",
    "print(lstm_f2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is very sparse and we need to run more epochs, and also a larger dataset. So the resulting MSE's are not so satisfying. However, we can still compare them and draw some early conclusions.\n",
    "\n",
    "1. RNN works better than CNN. A possible reason might be that reviews are sequential data.\n",
    "2. LSTM works better than GRU. More epochs will lead to better performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
